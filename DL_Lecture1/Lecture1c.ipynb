{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNb4Oybzfqmo",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Las310Xgwpn",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def showOpencvImage(image, isGray=False):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4739,
     "status": "ok",
     "timestamp": 1583392351629,
     "user": {
      "displayName": "Zbisław Tabor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihHgliefc3mumKqaCxhwnyI891sHQDAZ2h7sAhfw=s64",
      "userId": "16921256151317999512"
     },
     "user_tz": -60
    },
    "id": "dpK1YIhvhqWH",
    "outputId": "84cd89b5-fc80-438a-ffba-a82a38be16b5",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#Mount Google Drive\n",
    "!pwd\n",
    "!ls drive/My\\ Drive/Colab\\ Notebooks/Lectures/Lecture1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbJzh9-lgxWX"
   },
   "source": [
    "# Linear discrimination\n",
    "In the example below discrimination between classes is made based on the value of a linear scoring function:\n",
    "\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig1.png)\n",
    "\n",
    "where xi is an object descriptor (feature vector)\n",
    "# Classical approaches based on linear discrimination boudaries:\n",
    "LDA\n",
    "\n",
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1583394449782,
     "user": {
      "displayName": "Zbisław Tabor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihHgliefc3mumKqaCxhwnyI891sHQDAZ2h7sAhfw=s64",
      "userId": "16921256151317999512"
     },
     "user_tz": -60
    },
    "id": "uN-Ffk_zg1ZQ",
    "outputId": "71b304e5-0bee-4831-d074-06206eb8a2f9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0625dd7b5f47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# \"feature vector\" representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beagle.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4045: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# initialize the class labels and set the seed of the pseudorandom\n",
    "# number generator so we can reproduce our results\n",
    "labels = [\"dog\", \"cat\", \"panda\"]\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weight matrix and bias vector -- in a\n",
    "# *real* training and classification task, these parameters would\n",
    "# be *learned* by our model, but for the sake of this example,\n",
    "# let's use random values\n",
    "W = np.random.randn(3, 3072)\n",
    "b = np.random.randn(3)\n",
    "\n",
    "# load our example image, resize it, and then flatten it into our\n",
    "# \"feature vector\" representation\n",
    "orig = cv2.imread(\"beagle.png\")\n",
    "image = cv2.resize(orig, (32, 32)).flatten()\n",
    "\n",
    "print(image.shape)\n",
    "print(W.shape)\n",
    "print(b.shape)\n",
    "\n",
    "# compute the output scores by taking the dot product between the\n",
    "# weight matrix and image pixels, followed by adding in the bias\n",
    "scores = W.dot(image) + b\n",
    "\n",
    "# loop over the scores + labels and display them\n",
    "for (label, score) in zip(labels, scores):\n",
    "\tprint(\"[INFO] {}: {:.2f}\".format(label, score))\n",
    "\n",
    "# draw the label with the highest score on the image as our\n",
    "# prediction\n",
    "cv2.putText(orig, \"Label: {}\".format(labels[np.argmax(scores)]),\n",
    "\t(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "#im = [orig[:,:,i] for i in [2,1,0]]\n",
    "#for i in range(0,3):\n",
    "#    np.copyto(orig[:,:,i], im[i])\n",
    "\n",
    "# display our input image\n",
    "\n",
    "showOpencvImage(orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWG0h-4TlBHI"
   },
   "source": [
    "# Discriminating functions for feed forward deep learning\n",
    "\n",
    "Output = Activation_N ( W_N* (Activation_(N-1)(W_(N_-1)\n",
    "(\n",
    "....\n",
    "Activation_1 (W_1*Input)\n",
    "...\n",
    ") ) ) )\n",
    "\n",
    "While activations are elements of a model (are fixed), the model matrices W_i are learnt from data based on some optimization procedure.\n",
    "\n",
    "The kind of optimization here is **supervised learning**.\n",
    "A well known example of an supervised learning approach is linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUz-Iddio6xV"
   },
   "source": [
    "# Activations\n",
    "\n",
    "Activations used in deep learning are **differentiable**.\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NytmIhEyrYk8"
   },
   "source": [
    "# Data for supervised learning\n",
    "\n",
    "1.   Training input data + annotations\n",
    "2.   Validation data for model selection + annotations\n",
    "3.   Testing data + annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGzEAoBOyCib"
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "In the next example, we find a matrix of weights W that minimizes the loss function\n",
    "\n",
    "(y_true-y_pred) ** 2\n",
    "\n",
    "where ypred = trainX.dot (W)\n",
    "\n",
    "trainX has three components (x1, x2,1), We also has three components (W1, W2, W3), x1 and x2 are coordinates of the object on the plane\n",
    "but trainX.dot (W) is the marked distance of the point (x1, x2) from the straight line W1 * x1 + W2 * x2 + W3 = 0 (point above the straight line or below the straight line).\n",
    "\n",
    "The optimization procedure is therefore looking for a straight line separating the two classes, so that the accuracy of the classification is the best.\n",
    "\n",
    "This is similar to Linear SVM or LDA classifiers\n",
    "\n",
    "In many dimensions, the product of traiX.doc (W) is still interpreted as a distance from hyperplane.\n",
    "\n",
    "**We will derive the equation for gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeMUOenTsnKD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "\t# compute the sigmoid activation value for a given input\n",
    "\treturn 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "\t# compute the derivative of the sigmoid function ASSUMING\n",
    "\t# that the input `x` has already been passed through the sigmoid\n",
    "\t# activation function y = 1/(1+exp(-x)) dy/dx = (1-y)*y\n",
    "\treturn x * (1 - x)\n",
    "\n",
    "def predict(X, W):\n",
    "\t# take the dot product between our features and weight matrix\n",
    "\tpreds = sigmoid_activation(X.dot(W))\n",
    "\n",
    "\t# apply a step function to threshold the outputs to binary\n",
    "\t# class labels\n",
    "\tpreds[preds <= 0.5] = 0\n",
    "\tpreds[preds > 0] = 1\n",
    "\n",
    "\t# return the predictions\n",
    "\treturn preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1646,
     "status": "ok",
     "timestamp": 1583395845400,
     "user": {
      "displayName": "Zbisław Tabor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihHgliefc3mumKqaCxhwnyI891sHQDAZ2h7sAhfw=s64",
      "userId": "16921256151317999512"
     },
     "user_tz": -60
    },
    "id": "uu9QeMM1o5_E",
    "outputId": "49dd0ac4-a7ad-479f-b17d-201cebba0a0a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# generate a 2-class classification problem with 1,000 data points,\n",
    "# where each data point is a 2D feature vector\n",
    "(X, y) = make_blobs(n_samples=1000, n_features=2, centers=2,\n",
    "\tcluster_std=1.5, random_state=1)\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "print(np.min(y))\n",
    "print(np.max(y))\n",
    "\n",
    "# insert a column of 1's as the last entry in the feature\n",
    "# matrix -- this little trick allows us to treat the bias\n",
    "# as a trainable parameter within the weight matrix\n",
    "X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "# partition the data into training and testing splits using 50% of\n",
    "# the data for training and the remaining 50% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y,\n",
    "\ttest_size=0.5, random_state=42)\n",
    "\n",
    "# initialize our weight matrix and list of losses\n",
    "print(\"[INFO] training...\")\n",
    "W = np.random.randn(X.shape[1], 1)\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# loop over the desired number of epochs\n",
    "for epoch in np.arange(0, EPOCHS):\n",
    "\t# take the dot product between our features `X` and the weight\n",
    "\t# matrix `W`, then pass this value through our sigmoid activation\n",
    "\t# function, thereby giving us our predictions on the dataset\n",
    "\tpreds = sigmoid_activation(trainX.dot(W))\n",
    "\n",
    "\t# now that we have our predictions, we need to determine the\n",
    "\t# `error`, which is the difference between our predictions and\n",
    "\t# the true values\n",
    "\terror = preds - trainY\n",
    "\tloss = np.sum(error ** 2)\n",
    "\tlosses.append(loss)\n",
    "\n",
    "\t# the gradient descent update is the dot product between our\n",
    "\t# (1) features and (2) the error of the sigmoid derivative of\n",
    "\t# our predictions\n",
    "\td = error * sigmoid_deriv(preds)\n",
    "\tgradient = trainX.T.dot(d)\n",
    "\n",
    "\t# in the update stage, all we need to do is \"nudge\" the weight\n",
    "\t# matrix in the negative direction of the gradient (hence the\n",
    "\t# term \"gradient descent\" by taking a small step towards a set\n",
    "\t# of \"more optimal\" parameters\n",
    "\tW += -LEARNING_RATE * gradient\n",
    "\n",
    "\t# check to see if an update should be displayed\n",
    "\tif epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(int(epoch + 1),\n",
    "\t\t\tloss))\n",
    "\n",
    "# evaluate our model\n",
    "print(\"[INFO] evaluating...\")\n",
    "preds = predict(testX, W)\n",
    "print(classification_report(testY, preds))\n",
    "\n",
    "print(\"Model matrix \",W)\n",
    "\n",
    "# plot the (testing) classification data\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(testX[:, 0], testX[:, 1], marker=\"o\", c=testY[:, 0], s=30)\n",
    "f1 = np.arange(-15,2.5,0.5)\n",
    "plt.plot(f1,-W[0]/W[1]*f1-W[2]/W[1])\n",
    "\n",
    "# construct a figure that plots the loss over time\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9uKruPtyrVX"
   },
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8pz2w75yuco",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def next_batch(X, y, batchSize):\n",
    "\t# loop over our dataset `X` in mini-batches, yielding a tuple of\n",
    "\t# the current batched data and labels\n",
    "\tfor i in np.arange(0, X.shape[0], batchSize):\n",
    "\t\tyield (X[i:i + batchSize], y[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2245,
     "status": "ok",
     "timestamp": 1583396663805,
     "user": {
      "displayName": "Zbisław Tabor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihHgliefc3mumKqaCxhwnyI891sHQDAZ2h7sAhfw=s64",
      "userId": "16921256151317999512"
     },
     "user_tz": -60
    },
    "id": "Oniu7lLAyxuB",
    "outputId": "6f3f7405-06a4-4d4f-8acd-4f2b10367340",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "(X, y) = make_blobs(n_samples=1000, n_features=2, centers=2,cluster_std=1.5, random_state=1)\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "# insert a column of 1's as the last entry in the feature\n",
    "# matrix -- this little trick allows us to treat the bias\n",
    "# as a trainable parameter within the weight matrix\n",
    "X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "# partition the data into training and testing splits using 50% of\n",
    "# the data for training and the remaining 50% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y,test_size=0.5, random_state=42)\n",
    "\n",
    "# initialize our weight matrix and list of losses\n",
    "print(\"[INFO] training...\")\n",
    "W = np.random.randn(X.shape[1], 1)\n",
    "losses = []\n",
    "\n",
    "# loop over the desired number of epochs\n",
    "for epoch in np.arange(0, EPOCHS):\n",
    "\t# initialize the total loss for the epoch\n",
    "\tepochLoss = []\n",
    "\n",
    "\t# loop over our data in batches\n",
    "\tfor (batchX, batchY) in next_batch(trainX, trainY, BATCH_SIZE):\n",
    "\t\t# take the dot product between our current batch of features\n",
    "\t\t# and the weight matrix, then pass this value through our\n",
    "\t\t# activation function\n",
    "\t\tpreds = sigmoid_activation(batchX.dot(W))\n",
    "\n",
    "\t\t# now that we have our predictions, we need to determine the\n",
    "\t\t# `error`, which is the difference between our predictions\n",
    "\t\t# and the true values\n",
    "\t\terror = preds - batchY\n",
    "\t\tepochLoss.append(np.sum(error ** 2))\n",
    "\n",
    "\t\t# the gradient descent update is the dot product between our\n",
    "\t\t# (1) current batch and (2) the error of the sigmoid\n",
    "\t\t# derivative of our predictions\n",
    "\t\td = error * sigmoid_deriv(preds)\n",
    "\t\tgradient = batchX.T.dot(d)\n",
    "\n",
    "\t\t# in the update stage, all we need to do is \"nudge\" the\n",
    "\t\t# weight matrix in the negative direction of the gradient\n",
    "\t\t# (hence the term \"gradient descent\" by taking a small step\n",
    "\t\t# towards a set of \"more optimal\" parameters\n",
    "\t\tW += -LEARNING_RATE * gradient\n",
    "\n",
    "\t# update our loss history by taking the average loss across all\n",
    "\t# batches\n",
    "\tloss = np.average(epochLoss)\n",
    "\tlosses.append(loss)\n",
    "\n",
    "\t# check to see if an update should be displayed\n",
    "\tif epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(int(epoch + 1),\n",
    "\t\t\tloss))\n",
    "\n",
    "# evaluate our model\n",
    "print(\"[INFO] evaluating...\")\n",
    "preds = predict(testX, W)\n",
    "print(classification_report(testY, preds))\n",
    "\n",
    "# plot the (testing) classification data\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.title(\"Data\")\n",
    "plt.scatter(testX[:, 0], testX[:, 1], marker=\"o\", c=testY[:, 0], s=30)\n",
    "\n",
    "# construct a figure that plots the loss over time\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud5nrE60zCT_"
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "# Hinge loss function for multi-class classification\n",
    "\n",
    "For each i-th object its class score s_i,j with respect to j-th class is computed and then the loss functio is calculated. In the equation s_yi is the score with respect to the true class y_i of i-th object. \n",
    "\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig3.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig4.png)\n",
    "\n",
    "**Loss for dog**\n",
    "\n",
    "max(0, 1.33 - 4.26 + 1) + max(0, -1.01 - 4.26 + 1) = 0\n",
    "\n",
    "**Loss for cat**\n",
    "\n",
    "max(0, 3.76 - (-1.20) + 1) + max(0, -3.81 - (-1.20) + 1) = 5,96\n",
    "\n",
    "**Loss for panda:**\n",
    "\n",
    "max(0, -2.37 - (-2.27) + 1) + max(0, 1.03 - (-2.27) + 1) = 5.2\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Cross entropy loss\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig5.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPhMjMGvFftK"
   },
   "source": [
    "# Further topics concerning gradient descent\n",
    "\n",
    "https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
    "\n",
    "Momentum and esterov correction\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNGaII0EG0pp"
   },
   "source": [
    "# Regularization\n",
    "\n",
    "![alt text](https://sites.google.com/site/zbislawtabor/dydaktyka/ml/Fig8.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPagS2pALC5fV9Dg4kMQIfs",
   "collapsed_sections": [],
   "mount_file_id": "1MXi6V6kN91419lOTdmHVsCLyCzgxqsBx",
   "name": "Lecture1c.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xPp3Npx_Gsnq",
    "outputId": "50398b17-2a3f-4ae8-ab13-14b69b725b50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9B1NVcGgGsnu"
   },
   "source": [
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
    "But what happens when you have more than two classes? \n",
    "\n",
    "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
    "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_BxBw30Gsnv"
   },
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXRRZP7YGsnv"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID983tScGsnz"
   },
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TVgXbpJSGsn0",
    "outputId": "5d2eb112-c490-4228-e2dc-7b7dbf30a3cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rs9Ib7igGsn5",
    "outputId": "2d10cf4b-444e-4d49-c648-936ba5a10745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epyllaGxGsn9"
   },
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "pzJeWF7IGsn9",
    "outputId": "0a0855d2-0102-4707-e2a3-84fbd511b846"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqWHoQuMGsoB"
   },
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LVwO7EJGsoC"
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8b9SzsPHGsoH",
    "outputId": "ecfc870b-723a-429b-cd06-9c720c200faa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiMFkehoGsoM"
   },
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DY_YLQ_9GsoN",
    "outputId": "52a7dffa-cecf-45c7-9316-110f3b32d090"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wgH2f_YGsoQ"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peAnMFfWGsoR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnCPYOcRGsoU"
   },
   "source": [
    "\n",
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "5e6qNKncGsoV",
    "outputId": "5f266e44-e736-4fcd-bcc0-4ba4c1ed6c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 46)\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot1(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = []\n",
    "    \n",
    "    for label in labels:\n",
    "        results.append(np.asarray([label==i for i in range(0,dimension)],dtype=np.uint8))\n",
    "        \n",
    "    return np.asarray(results)\n",
    "    \n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n",
    "\n",
    "print(one_hot_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BA80u_h9Gsoa",
    "outputId": "e0efcf6e-c575-4be0-8568-e650375ac3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "dimension = 46\n",
    "label = 34\n",
    "res = np.asarray([label==i for i in range(0,dimension)],dtype=np.uint8)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKnXjy32Gsod"
   },
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vKvK8hYGsoe"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn7rPtHoGsoh"
   },
   "source": [
    "## Class weights\n",
    "\n",
    "Before we continue, let us calculate the weights for the classess. Fit method for a network model accepts an optional argument class_weight which must be a disctionary. All classess present in the training set must have associated items in the dictionary.\n",
    "\n",
    "## Use Keras home page!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "nDZAa-uRGsoi",
    "outputId": "e8e69628-7d9e-4f11-a6d6-c8993448a68a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 46)\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n",
      "[0.01245548 0.00158577 0.00925745 0.00021686 0.00035149 0.04029715\n",
      " 0.01427191 0.04281572 0.00492843 0.00678269 0.00552461 0.00175654\n",
      " 0.01398064 0.00398286 0.02634814 0.03425258 0.00154291 0.01756542\n",
      " 0.01037957 0.00124782 0.00254666 0.00685052 0.0456701  0.01670857\n",
      " 0.01104922 0.00744621 0.02854382 0.0456701  0.01427191 0.03605535\n",
      " 0.01522337 0.01756542 0.02140786 0.06227742 0.01370103 0.06850516\n",
      " 0.01398064 0.03605535 0.03605535 0.02854382 0.01902921 0.02283505\n",
      " 0.05269627 0.0326215  0.05708763 0.03805842]\n",
      "{0: 0.012455483067718345, 1: 0.0015857675201956226, 2: 0.009257453631412284, 3: 0.00021685709677888857, 4: 0.0003514887474215028, 5: 0.040297151101441706, 6: 0.014271907681760603, 7: 0.04281572304528181, 8: 0.004928428551974885, 9: 0.0067826887992525646, 10: 0.005524609425197653, 11: 0.0017565424839089973, 12: 0.013980644259683856, 13: 0.0039828579577006334, 14: 0.02634813725863496, 15: 0.03425257843622545, 16: 0.001542908938568714, 17: 0.017565424839089974, 18: 0.010379569223098621, 19: 0.0012478170650719654, 20: 0.002546660106782561, 21: 0.00685051568724509, 22: 0.04567010458163393, 23: 0.016708574846939245, 24: 0.011049218850395306, 25: 0.007446212703527272, 26: 0.028543815363521206, 27: 0.04567010458163393, 28: 0.014271907681760603, 29: 0.03605534572234258, 30: 0.015223368193877979, 31: 0.017565424839089974, 32: 0.021407861522640906, 33: 0.06227741533859173, 34: 0.01370103137449018, 35: 0.0685051568724509, 36: 0.013980644259683856, 37: 0.03605534572234258, 38: 0.03605534572234258, 39: 0.028543815363521206, 40: 0.01902921024234747, 41: 0.022835052290816966, 42: 0.05269627451726992, 43: 0.032621503272595664, 44: 0.05708763072704241, 45: 0.03805842048469494}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = np.unique(train_labels,return_counts=True)\n",
    "\n",
    "counts = np.asarray(counts)\n",
    "print(counts.shape)\n",
    "\n",
    "print(counts)\n",
    "\n",
    "weights = np.ones(counts.shape[1],dtype=np.float32)\n",
    "\n",
    "weights = 1/counts[1,:]\n",
    "\n",
    "w = np.sum(weights)\n",
    "\n",
    "weights = weights/w\n",
    "\n",
    "print(weights)\n",
    "\n",
    "weightsDict = dict(zip(np.asarray(counts[0,:]),weights))\n",
    "print(weightsDict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGCcIqI0Gsok"
   },
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygd4H3wFGsok"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVtwdAcbGsom"
   },
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKTHHdpGGsoo"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnJbO-nBGsot"
   },
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSnD6zqhGsou"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywewXCcgGsoy"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "xjhvA48gGsoy",
    "outputId": "568d0c13-50cb-405e-c74c-353edc7855dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.0130 - accuracy: 0.2890 - val_loss: 3.4093 - val_accuracy: 0.5470\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.0104 - accuracy: 0.6619 - val_loss: 2.7379 - val_accuracy: 0.6940\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.0075 - accuracy: 0.7744 - val_loss: 2.2082 - val_accuracy: 0.7400\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 0.0052 - accuracy: 0.8188 - val_loss: 1.8369 - val_accuracy: 0.7520\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.0036 - accuracy: 0.8436 - val_loss: 1.5791 - val_accuracy: 0.7620\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 0.0025 - accuracy: 0.8659 - val_loss: 1.3351 - val_accuracy: 0.7970\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.0018 - accuracy: 0.8852 - val_loss: 1.2545 - val_accuracy: 0.8050\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 0.0013 - accuracy: 0.8989 - val_loss: 1.0981 - val_accuracy: 0.8110\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 9.8121e-04 - accuracy: 0.9121 - val_loss: 1.0588 - val_accuracy: 0.8140\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 7.6688e-04 - accuracy: 0.9198 - val_loss: 1.0001 - val_accuracy: 0.8170\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 5.9938e-04 - accuracy: 0.9271 - val_loss: 0.9870 - val_accuracy: 0.8240\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 5.0729e-04 - accuracy: 0.9341 - val_loss: 0.9758 - val_accuracy: 0.8230\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 4.7327e-04 - accuracy: 0.9390 - val_loss: 1.0061 - val_accuracy: 0.8240\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 130us/step - loss: 3.8244e-04 - accuracy: 0.9427 - val_loss: 0.9548 - val_accuracy: 0.8280\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 3.4161e-04 - accuracy: 0.9469 - val_loss: 0.9539 - val_accuracy: 0.8280\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 3.0478e-04 - accuracy: 0.9493 - val_loss: 0.9832 - val_accuracy: 0.8300\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 2.9010e-04 - accuracy: 0.9519 - val_loss: 0.9574 - val_accuracy: 0.8280\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 3.0726e-04 - accuracy: 0.9543 - val_loss: 0.9552 - val_accuracy: 0.8300\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 2.4756e-04 - accuracy: 0.9550 - val_loss: 0.9934 - val_accuracy: 0.8240\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 2.7149e-04 - accuracy: 0.9558 - val_loss: 1.0059 - val_accuracy: 0.8280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),class_weight = weightsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2UHyhZYGso1"
   },
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "UikNyCnXGso1",
    "outputId": "be29a658-5a71-4d31-e217-b24b2bd06687"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "flCuMSkLGso4",
    "outputId": "929693de-0651-4cc6-95ea-07d3db35cf2f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rqwds0jvGso8"
   },
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "Q6q8W05tGso8",
    "outputId": "b287e645-ec9e-4402-99eb-7806710e3c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 2.6490 - accuracy: 0.5352 - val_loss: 1.7492 - val_accuracy: 0.6420\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 1.4202 - accuracy: 0.7098 - val_loss: 1.3053 - val_accuracy: 0.7240\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 1.0453 - accuracy: 0.7795 - val_loss: 1.1448 - val_accuracy: 0.7460\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 125us/step - loss: 0.8149 - accuracy: 0.8232 - val_loss: 1.0266 - val_accuracy: 0.7820\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.6454 - accuracy: 0.8649 - val_loss: 0.9782 - val_accuracy: 0.8000\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 0.5176 - accuracy: 0.8966 - val_loss: 0.9299 - val_accuracy: 0.7950\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.4209 - accuracy: 0.9168 - val_loss: 0.9094 - val_accuracy: 0.8070\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.3344 - accuracy: 0.9286 - val_loss: 0.9246 - val_accuracy: 0.8030\n",
      "2246/2246 [==============================] - 0s 87us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pwPf5E8LGspB",
    "outputId": "5a5ab8ff-51b9-43f0-fdf4-581221ac26e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0040374812757131, 0.777827262878418]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hLqfooeGspE"
   },
   "source": [
    "\n",
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zxOmYBHTGspF",
    "outputId": "09136f6b-ce21-4e12-d2fc-687c6aafe032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18432769367764915"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC7lBXKHGspK"
   },
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUoxJfxyGspK"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KeKIE79FGspN"
   },
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R4EJqv99GspN",
    "outputId": "005df747-7151-4f7d-e896-fa4156f012af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n87w89WgGspQ"
   },
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MgjZL8B3GspR",
    "outputId": "ce575b20-cb7f-49b3-fc9e-f0fdfcecd82a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmE_yckOGspU"
   },
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NeHNBXZiGspV",
    "outputId": "83dbbed2-684e-4f85-e3dc-5728c5fe3f6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcwF8DUlGspX"
   },
   "source": [
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Js3owkCVGspX"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGS0xqs4Gspe"
   },
   "source": [
    "\n",
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLvKeuubGspe"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLXQ9eh7Gspj"
   },
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrAHxbi0Gspj"
   },
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "AlWbfgWvGspj",
    "outputId": "7e42871c-f40d-4d1b-c199-a38a8eac1ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 173us/step - loss: 2.8662 - accuracy: 0.2711 - val_loss: 2.1979 - val_accuracy: 0.4160\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 168us/step - loss: 1.8654 - accuracy: 0.4669 - val_loss: 1.6804 - val_accuracy: 0.5780\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.4618 - accuracy: 0.6407 - val_loss: 1.4901 - val_accuracy: 0.6430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 1.2797 - accuracy: 0.6670 - val_loss: 1.4329 - val_accuracy: 0.6560\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 1.1702 - accuracy: 0.6852 - val_loss: 1.4150 - val_accuracy: 0.6540\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 1.0905 - accuracy: 0.7008 - val_loss: 1.4193 - val_accuracy: 0.6680\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 1.0245 - accuracy: 0.7126 - val_loss: 1.4356 - val_accuracy: 0.6720\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.9700 - accuracy: 0.7205 - val_loss: 1.4394 - val_accuracy: 0.6660\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.9227 - accuracy: 0.7303 - val_loss: 1.4822 - val_accuracy: 0.6710\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.8853 - accuracy: 0.7414 - val_loss: 1.4809 - val_accuracy: 0.6720\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.8476 - accuracy: 0.7484 - val_loss: 1.5428 - val_accuracy: 0.6720\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 0.8175 - accuracy: 0.7541 - val_loss: 1.5676 - val_accuracy: 0.6740\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.7899 - accuracy: 0.7671 - val_loss: 1.6165 - val_accuracy: 0.6660\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 159us/step - loss: 0.7635 - accuracy: 0.7705 - val_loss: 1.6932 - val_accuracy: 0.6730\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 159us/step - loss: 0.7372 - accuracy: 0.7831 - val_loss: 1.7082 - val_accuracy: 0.6720\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.7130 - accuracy: 0.7982 - val_loss: 1.7360 - val_accuracy: 0.6770\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 0.6911 - accuracy: 0.8058 - val_loss: 1.7982 - val_accuracy: 0.6790\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.6689 - accuracy: 0.8079 - val_loss: 1.8758 - val_accuracy: 0.6820\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.6471 - accuracy: 0.8099 - val_loss: 1.9402 - val_accuracy: 0.6750\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.6304 - accuracy: 0.8121 - val_loss: 2.0255 - val_accuracy: 0.6770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff8314b7dd8>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4RgPiCeGspm"
   },
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt1JSFbvGspm"
   },
   "source": [
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EE5prPFGspn"
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iHbpIcFGspn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gr_CtccMGsps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUsOkmyRK314"
   },
   "source": [
    "# Zadanie 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VwIs1ivYThO"
   },
   "source": [
    "Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lv9pP_HoYVHs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jo-Sm1tYWV7"
   },
   "source": [
    "# Base params\n",
    "2 hidden layers, 64 layer size, categorical_crossentropy loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V80VQuomLczB",
    "outputId": "70b87464-d1eb-4597-9607-ba0fd2a408ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 145us/step - loss: 2.5442 - accuracy: 0.5353 - val_loss: 1.6537 - val_accuracy: 0.6350\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 1.3520 - accuracy: 0.7090 - val_loss: 1.2624 - val_accuracy: 0.7160\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 1.0155 - accuracy: 0.7769 - val_loss: 1.1131 - val_accuracy: 0.7470\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.8034 - accuracy: 0.8279 - val_loss: 1.0107 - val_accuracy: 0.7780\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.6389 - accuracy: 0.8707 - val_loss: 0.9637 - val_accuracy: 0.8050\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.5121 - accuracy: 0.8979 - val_loss: 0.9259 - val_accuracy: 0.8040\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 135us/step - loss: 0.4114 - accuracy: 0.9167 - val_loss: 0.8918 - val_accuracy: 0.8190\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 0.3341 - accuracy: 0.9316 - val_loss: 0.9771 - val_accuracy: 0.7860\n",
      "confusion matrix:\n",
      "[[ 7  2  0 ...  0  0  0]\n",
      " [ 0 84  0 ...  0  0  0]\n",
      " [ 0  3 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  4  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.64      0.80      0.71       105\n",
      "           2       0.75      0.60      0.67        20\n",
      "           3       0.81      0.97      0.88       813\n",
      "           4       0.90      0.75      0.82       474\n",
      "           6       1.00      0.79      0.88        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.73      0.63      0.68        38\n",
      "           9       0.81      0.68      0.74        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.58      0.73      0.65        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.59      0.70      0.64        37\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.72      0.74      0.73        99\n",
      "          17       1.00      0.17      0.29        12\n",
      "          18       0.69      0.55      0.61        20\n",
      "          19       0.62      0.74      0.67       133\n",
      "          20       0.61      0.50      0.55        70\n",
      "          21       0.57      0.63      0.60        27\n",
      "          23       0.50      0.42      0.45        12\n",
      "          24       0.46      0.32      0.37        19\n",
      "          25       0.83      0.48      0.61        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.20      0.29        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.67      0.57        12\n",
      "          31       0.80      0.31      0.44        13\n",
      "          32       0.78      0.70      0.74        10\n",
      "          34       0.83      0.71      0.77         7\n",
      "          36       0.22      0.18      0.20        11\n",
      "          39       0.00      0.00      0.00         5\n",
      "          41       0.50      0.12      0.20         8\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.77      0.78      0.77      2202\n",
      "   macro avg       0.67      0.51      0.55      2202\n",
      "weighted avg       0.77      0.78      0.76      2202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcXK1BR6ayFP"
   },
   "source": [
    "precision: 0.77, f1-score: 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vd9maxDua6ZT"
   },
   "source": [
    "1 hidden layer, 64 layer size, categorical_crossentropy loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V2SNG3JRa-BI",
    "outputId": "14c9e9f5-e4f9-4693-9f37-13929e3b7eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 141us/step - loss: 2.6187 - accuracy: 0.5646 - val_loss: 1.8481 - val_accuracy: 0.6440\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 135us/step - loss: 1.4900 - accuracy: 0.7144 - val_loss: 1.3449 - val_accuracy: 0.7270\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 1.0753 - accuracy: 0.7861 - val_loss: 1.1307 - val_accuracy: 0.7630\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.8364 - accuracy: 0.8332 - val_loss: 1.0062 - val_accuracy: 0.8030\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 135us/step - loss: 0.6721 - accuracy: 0.8710 - val_loss: 0.9330 - val_accuracy: 0.8060\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 0.5486 - accuracy: 0.8923 - val_loss: 0.8863 - val_accuracy: 0.8170\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 134us/step - loss: 0.4527 - accuracy: 0.9080 - val_loss: 0.8577 - val_accuracy: 0.8180\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.3776 - accuracy: 0.9217 - val_loss: 0.8318 - val_accuracy: 0.8240\n",
      "confusion matrix:\n",
      "[[ 7  2  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  4 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  2  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.62      0.82      0.71       105\n",
      "           2       0.80      0.60      0.69        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.77      0.91      0.84       474\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.73      0.63      0.68        38\n",
      "           9       0.81      0.68      0.74        25\n",
      "          10       0.86      0.83      0.85        30\n",
      "          11       0.56      0.81      0.66        83\n",
      "          12       0.33      0.08      0.12        13\n",
      "          13       0.57      0.62      0.60        37\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.73      0.78      0.75        99\n",
      "          17       1.00      0.08      0.15        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.67      0.72      0.69       133\n",
      "          20       0.73      0.47      0.57        70\n",
      "          21       0.67      0.81      0.73        27\n",
      "          23       0.57      0.33      0.42        12\n",
      "          24       0.50      0.16      0.24        19\n",
      "          25       0.85      0.55      0.67        31\n",
      "          26       1.00      0.75      0.86         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.20      0.10      0.13        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.43      0.23      0.30        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          41       0.50      0.12      0.20         8\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.79      0.80      0.80      2213\n",
      "   macro avg       0.70      0.48      0.54      2213\n",
      "weighted avg       0.79      0.80      0.78      2213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yDc-f6fbPuV"
   },
   "source": [
    "precision: 0.79, f1-score: 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wt1MiVaHcNUm"
   },
   "source": [
    "3 hidden layers, 64 layer size, categorical_crossentropy loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UyF7GgSscP_G",
    "outputId": "0e3ac365-be73-45c7-db19-50d0e030533d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 151us/step - loss: 2.6020 - accuracy: 0.4981 - val_loss: 1.7184 - val_accuracy: 0.5830\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 1.4253 - accuracy: 0.6824 - val_loss: 1.2971 - val_accuracy: 0.6960\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 135us/step - loss: 1.0818 - accuracy: 0.7552 - val_loss: 1.1618 - val_accuracy: 0.7340\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.8549 - accuracy: 0.8064 - val_loss: 1.0433 - val_accuracy: 0.7700\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.6855 - accuracy: 0.8464 - val_loss: 0.9892 - val_accuracy: 0.7850\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 132us/step - loss: 0.5476 - accuracy: 0.8742 - val_loss: 0.9885 - val_accuracy: 0.7820\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 0.4355 - accuracy: 0.9028 - val_loss: 1.0115 - val_accuracy: 0.7880\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 0.3585 - accuracy: 0.9201 - val_loss: 0.9539 - val_accuracy: 0.8060\n",
      "confusion matrix:\n",
      "[[ 6  3  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  4 13 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  4  0  0]\n",
      " [ 0  1  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57        12\n",
      "           1       0.57      0.82      0.67       105\n",
      "           2       0.76      0.65      0.70        20\n",
      "           3       0.90      0.95      0.92       813\n",
      "           4       0.79      0.90      0.84       474\n",
      "           6       0.76      0.93      0.84        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.63      0.66        38\n",
      "           9       0.84      0.64      0.73        25\n",
      "          10       0.83      0.80      0.81        30\n",
      "          11       0.60      0.75      0.67        83\n",
      "          12       0.33      0.08      0.12        13\n",
      "          13       0.58      0.49      0.53        37\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.66      0.76      0.70        99\n",
      "          17       1.00      0.17      0.29        12\n",
      "          18       0.48      0.65      0.55        20\n",
      "          19       0.62      0.68      0.65       133\n",
      "          20       0.59      0.50      0.54        70\n",
      "          21       0.70      0.59      0.64        27\n",
      "          23       0.25      0.08      0.12        12\n",
      "          24       0.50      0.16      0.24        19\n",
      "          25       0.70      0.52      0.59        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.50      0.17      0.25        12\n",
      "          31       0.50      0.15      0.24        13\n",
      "          32       1.00      0.40      0.57        10\n",
      "          34       0.80      0.57      0.67         7\n",
      "          36       0.20      0.09      0.13        11\n",
      "          38       1.00      0.33      0.50         3\n",
      "          41       0.00      0.00      0.00         8\n",
      "          43       0.80      0.67      0.73         6\n",
      "\n",
      "   micro avg       0.77      0.79      0.78      2195\n",
      "   macro avg       0.63      0.44      0.48      2195\n",
      "weighted avg       0.76      0.79      0.76      2195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMAMB5HLdRgH"
   },
   "source": [
    "precision: 0.76, f1-score: 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYj387CTdZcH"
   },
   "source": [
    "2 hidden layers, 32 layer size, categorical_crossentropy loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0jFkp52OdVSq",
    "outputId": "8d8f2078-c52b-478e-c26c-7b39d375bce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 115us/step - loss: 3.0336 - accuracy: 0.3494 - val_loss: 2.2944 - val_accuracy: 0.5370\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 1.8918 - accuracy: 0.6254 - val_loss: 1.6309 - val_accuracy: 0.6490\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 1.4090 - accuracy: 0.7100 - val_loss: 1.3590 - val_accuracy: 0.6950\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 1.1531 - accuracy: 0.7567 - val_loss: 1.2226 - val_accuracy: 0.7360\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 0.9797 - accuracy: 0.7955 - val_loss: 1.1437 - val_accuracy: 0.7540\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.8449 - accuracy: 0.8210 - val_loss: 1.0894 - val_accuracy: 0.7700\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 107us/step - loss: 0.7311 - accuracy: 0.8449 - val_loss: 1.0427 - val_accuracy: 0.7820\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 107us/step - loss: 0.6338 - accuracy: 0.8643 - val_loss: 1.0096 - val_accuracy: 0.7850\n",
      "confusion matrix:\n",
      "[[ 6  2  0 ...  0  0  0]\n",
      " [ 0 88  0 ...  0  0  0]\n",
      " [ 0  3  9 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57        12\n",
      "           1       0.62      0.84      0.71       105\n",
      "           2       0.90      0.45      0.60        20\n",
      "           3       0.86      0.95      0.90       813\n",
      "           4       0.81      0.91      0.85       474\n",
      "           6       1.00      0.21      0.35        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.61      0.64        38\n",
      "           9       0.67      0.72      0.69        25\n",
      "          10       0.77      0.67      0.71        30\n",
      "          11       0.61      0.80      0.69        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.47      0.62      0.53        37\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.63      0.79      0.70        99\n",
      "          18       0.44      0.60      0.51        20\n",
      "          19       0.67      0.64      0.65       133\n",
      "          20       0.53      0.49      0.51        70\n",
      "          21       0.56      0.70      0.62        27\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.75      0.39      0.51        31\n",
      "          28       0.00      0.00      0.00        10\n",
      "          32       1.00      0.40      0.57        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          43       1.00      0.17      0.29         6\n",
      "\n",
      "   micro avg       0.76      0.80      0.78      2121\n",
      "   macro avg       0.56      0.45      0.47      2121\n",
      "weighted avg       0.74      0.80      0.76      2121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLNmDvNfdsYm"
   },
   "source": [
    "precision: 0.74, f1-score: 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSPYoWI6d0hh"
   },
   "source": [
    "2 hidden layers, 128 layer size, categorical_hinge loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A8-Rr72zd5ek",
    "outputId": "50c8db49-dd8b-411b-ab7b-447cd7344c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 197us/step - loss: 2.2512 - accuracy: 0.5661 - val_loss: 1.4220 - val_accuracy: 0.6860\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 177us/step - loss: 1.1176 - accuracy: 0.7568 - val_loss: 1.1096 - val_accuracy: 0.7510\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.7901 - accuracy: 0.8325 - val_loss: 0.9737 - val_accuracy: 0.7970\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.5725 - accuracy: 0.8814 - val_loss: 0.9189 - val_accuracy: 0.8070\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 177us/step - loss: 0.4216 - accuracy: 0.9118 - val_loss: 0.8833 - val_accuracy: 0.8230\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 179us/step - loss: 0.3153 - accuracy: 0.9328 - val_loss: 0.9281 - val_accuracy: 0.8090\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 177us/step - loss: 0.2523 - accuracy: 0.9396 - val_loss: 0.8797 - val_accuracy: 0.8190\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.2041 - accuracy: 0.9496 - val_loss: 0.9505 - val_accuracy: 0.8130\n",
      "confusion matrix:\n",
      "[[ 8  2  0 ...  0  0  0]\n",
      " [ 0 89  0 ...  0  0  0]\n",
      " [ 0  2 16 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  4  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.64      0.85      0.73       105\n",
      "           2       0.76      0.80      0.78        20\n",
      "           3       0.94      0.90      0.92       813\n",
      "           4       0.78      0.91      0.84       474\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.73      0.63      0.68        38\n",
      "           9       0.84      0.64      0.73        25\n",
      "          10       0.87      0.90      0.89        30\n",
      "          11       0.56      0.73      0.64        83\n",
      "          12       0.33      0.08      0.12        13\n",
      "          13       0.61      0.46      0.52        37\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.59      0.78      0.67        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.63      0.60      0.62        20\n",
      "          19       0.63      0.68      0.66       133\n",
      "          20       0.53      0.57      0.55        70\n",
      "          21       0.69      0.67      0.68        27\n",
      "          23       0.50      0.50      0.50        12\n",
      "          24       0.67      0.32      0.43        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.75      0.86         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.33      0.50      0.40         4\n",
      "          30       1.00      0.33      0.50        12\n",
      "          31       0.64      0.54      0.58        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.83      0.71      0.77         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.78      0.79      0.79      2226\n",
      "   macro avg       0.75      0.53      0.58      2226\n",
      "weighted avg       0.79      0.79      0.78      2226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zyfm63meBZt"
   },
   "source": [
    "precision: 0.79, f1-score: 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aDQ5aNJeE6i"
   },
   "source": [
    "2 hidden layers, 128 layer size, categorical_hinge loss function, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "h-wHaD2mfZlo",
    "outputId": "b140ebf0-8e56-48e1-990a-5edd8758c046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 151us/step - loss: 0.9843 - accuracy: 0.4739 - val_loss: 0.8381 - val_accuracy: 0.5680\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 131us/step - loss: 0.6569 - accuracy: 0.6277 - val_loss: 0.5677 - val_accuracy: 0.6520\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 129us/step - loss: 0.5139 - accuracy: 0.6967 - val_loss: 0.5112 - val_accuracy: 0.7060\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.4588 - accuracy: 0.7373 - val_loss: 0.4897 - val_accuracy: 0.7170\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.4092 - accuracy: 0.7620 - val_loss: 0.4681 - val_accuracy: 0.7350\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 129us/step - loss: 0.3698 - accuracy: 0.7897 - val_loss: 0.4472 - val_accuracy: 0.7510\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 132us/step - loss: 0.3359 - accuracy: 0.8158 - val_loss: 0.4320 - val_accuracy: 0.7720\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.3044 - accuracy: 0.8388 - val_loss: 0.4264 - val_accuracy: 0.7730\n",
      "confusion matrix:\n",
      "[[ 5  4  0 ...  0  0  0]\n",
      " [ 0 90  0 ...  0  0  0]\n",
      " [ 0  5  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.45        12\n",
      "           1       0.45      0.86      0.59       105\n",
      "           2       0.83      0.25      0.38        20\n",
      "           3       0.93      0.95      0.94       813\n",
      "           4       0.89      0.89      0.89       474\n",
      "           6       1.00      0.14      0.25        14\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.74      0.68      0.71        25\n",
      "          10       0.55      0.40      0.46        30\n",
      "          11       0.50      0.76      0.60        83\n",
      "          13       0.53      0.49      0.51        37\n",
      "          16       0.59      0.82      0.69        99\n",
      "          18       0.62      0.25      0.36        20\n",
      "          19       0.66      0.69      0.67       133\n",
      "          20       0.29      0.59      0.39        70\n",
      "          21       0.60      0.44      0.51        27\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.80      0.65      0.71        31\n",
      "\n",
      "   micro avg       0.75      0.82      0.78      2050\n",
      "   macro avg       0.62      0.55      0.55      2050\n",
      "weighted avg       0.78      0.82      0.79      2050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_hinge',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RQLRWohfiPW"
   },
   "source": [
    "precision: 0.78, f1-score: 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BplSih_5flye"
   },
   "source": [
    "2 hidden layers, 128 layer size, categorical_crossentropy loss function, tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CflWbSn8frLy",
    "outputId": "10e7d91e-17bf-4256-ce9f-1d23d42d2540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 138us/step - loss: 2.2814 - accuracy: 0.5426 - val_loss: 1.5813 - val_accuracy: 0.6440\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 131us/step - loss: 1.3139 - accuracy: 0.7264 - val_loss: 1.2317 - val_accuracy: 0.7360\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.9757 - accuracy: 0.8054 - val_loss: 1.0652 - val_accuracy: 0.7820\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.7505 - accuracy: 0.8624 - val_loss: 0.9497 - val_accuracy: 0.8110\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.5830 - accuracy: 0.8960 - val_loss: 0.8885 - val_accuracy: 0.8170\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.4563 - accuracy: 0.9211 - val_loss: 0.8510 - val_accuracy: 0.8190\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.3595 - accuracy: 0.9337 - val_loss: 0.8341 - val_accuracy: 0.8200\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 128us/step - loss: 0.2870 - accuracy: 0.9435 - val_loss: 0.8242 - val_accuracy: 0.8230\n",
      "confusion matrix:\n",
      "[[ 8  2  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  3 14 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  5  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification raport:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.69      0.82      0.75       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.94      0.93       813\n",
      "           4       0.81      0.88      0.84       474\n",
      "           6       0.81      0.93      0.87        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.68      0.66      0.67        38\n",
      "           9       0.83      0.60      0.70        25\n",
      "          10       0.83      0.83      0.83        30\n",
      "          11       0.58      0.71      0.64        83\n",
      "          12       0.33      0.08      0.12        13\n",
      "          13       0.57      0.62      0.60        37\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.69      0.80      0.74        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.57      0.60      0.59        20\n",
      "          19       0.68      0.67      0.68       133\n",
      "          20       0.64      0.56      0.60        70\n",
      "          21       0.66      0.70      0.68        27\n",
      "          23       0.58      0.58      0.58        12\n",
      "          24       0.58      0.37      0.45        19\n",
      "          25       0.84      0.68      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.20      0.10      0.13        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.60      0.25      0.35        12\n",
      "          31       0.89      0.62      0.73        13\n",
      "          32       0.89      0.80      0.84        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.71      0.71      0.71         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.18      0.24        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "\n",
      "   micro avg       0.79      0.81      0.80      2216\n",
      "   macro avg       0.72      0.55      0.59      2216\n",
      "weighted avg       0.79      0.81      0.79      2216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "predictions = model.predict(x_test);\n",
    "\n",
    "predicted_labels = np.array([np.argmax(predictions[i]) for i in range(0, predictions.shape[0])])\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "print(\"classification raport:\")\n",
    "print(classification_report(y_test, predicted_labels, labels=np.unique(predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5I6868tgFS-"
   },
   "source": [
    "precision: 0.79, f1-score: 0.79"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture2b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
